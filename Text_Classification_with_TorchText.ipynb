{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with TorchText.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWt+4x1ETErefzXFf4CQku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chuck2Win/text-generation---torchtext/blob/master/Text_Classification_with_TorchText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wSa4189Nh6d",
        "colab_type": "code",
        "outputId": "e85414f9-414f-42c2-9983-61a5b58eff7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "! pip install --upgrade torchtext #0.5.0으로 업그레이드"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 30.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40kB 39.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 51kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 61kB 35.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 71kB 28.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 28.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 34.6MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 39.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 41.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 30.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 32.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 25.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 27.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 28.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 29.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 29.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 29.5MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 29.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 29.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 29.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 29.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.85 torchtext-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSCVrlrJ_7Kb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "13f5dd02-d839-4386-a917-d1e3f77e3ab9"
      },
      "source": [
        "# Datasets supports the ngrams method\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.datasets import text_classification\n",
        "import os\n",
        "\n",
        "print(torchtext.__version__) # 0.31 -> upgrade 필요 -> upgrade\n",
        "print(torch.__version__) # 1.4.0 \n",
        "NGRAMS=2\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "train_dataset,test_dataset=text_classification.DATASETS['AG_NEWS'](root='./.data',ngrams=NGRAMS,vocab=None)\n",
        "# 이렇게 ngrams=NGRAMS로 설정하면, dataset은 a list of single words + bi-grams string\n",
        "BATCH_SIZE=16\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5.0\n",
            "1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "120000lines [00:07, 17042.01lines/s]\n",
            "120000lines [00:15, 7682.18lines/s]\n",
            "7600lines [00:00, 7944.43lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPDPMA83vmvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 공부용\n",
        "tt=nn.EmbeddingBag(10,3)\n",
        "tt.forward(torch.LongTensor([0,1,2,3]),torch.LongTensor([0]))\n",
        "list(tt.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivkeoBjyYFDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_noLsVSAXes",
        "colab_type": "text"
      },
      "source": [
        "EmbeddingBag : Computes sums or means of bags of embeddings without instantiating the intermediate embeddings\n",
        "(num of embeddings, embedding dim)\n",
        "\n",
        "inputs : input(Long Tensor), offsets(Long Tensor,optional)\n",
        "\n",
        "1) if input is 2D of shape (B,N)\n",
        "\n",
        "it will be treated as B bags(sequences) each of fixed length N, and this will return B\n",
        "\n",
        "values aggregated in a way depending on the mode. offsets is ignored and required to be\n",
        "\n",
        "None in this case\n",
        "\n",
        "\n",
        "**2)** If input is 1D of shape (N)\n",
        "It will be treated as a concatenation of multiple bags(sequences). offsets is required\n",
        "\n",
        "to be a 1D tensor containing the starting index positions of each bag in input. Therefore,\n",
        "\n",
        "for offsets of shape (B), input will be viewed as having (B) bags. Empty bags(having 0 length)\n",
        "\n",
        "will have returned vectors filled by zeros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812euJ6RZYOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AG_NEWS dataset\n",
        "# 1 : World\n",
        "# 2 : Sports\n",
        "# 3 : Business\n",
        "# 4 : Sci/Tec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3WIhDRJaHeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(train_dataset.get_vocab()) # including single word and ngrams\n",
        "embed_dim=32\n",
        "num_class=len(train_dataset.get_labels())\n",
        "model=TextSentiment(vocab_size,embed_dim,num_class).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lpbjbv-rar1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\n",
        "    # of elements in the dimension dim.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0) -> 1, 3, 6\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    # 가령 0,10,2,3 -> 즉 첫번째 문장의 길이가 10, 두번째가 2인 경우\n",
        "    # 0,10,12 이 batch의 시작이란 것을 알려준다는 것이다.\n",
        "\n",
        "    text = torch.cat(text)\n",
        "    return text, offsets, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYHTxv9sKG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62b8275d-a8c7-4aa6-ba2e-e3491a06261b"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10720"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3JUXMzuuAK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# torch.utils.datra.sampler.SubsetRandomSampler(index~)\n",
        "# torch.utils.data.DataLoader(~, sampler=train_indice)\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      collate_fn=generate_batch)\n",
        "    for i, (text, offsets, cls) in enumerate(data): # cls는 class 즉 label을 의미한다.\n",
        "        optimizer.zero_grad()\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        output = model(text, offsets)\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Adjust the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "def test(data_):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "    for text, offsets, cls in data:\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text, offsets)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return loss / len(data_), acc / len(data_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnZrC61KqbjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}